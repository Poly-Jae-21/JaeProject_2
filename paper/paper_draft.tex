%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\usepackage{array}
\usepackage{booktabs}
\usepackage[margin=1in]{geometry}
%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Nuclear Physics B}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Electric Vehicle Fast Charging Network Planning using Multi-Policy Deep Reinforcement Learning with Geospatial Analysis at a City Scale} %% Article title

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author{} %% Author name

%% Author affiliation
\affiliation{organization={},%Department and Organization
            addressline={}, 
            city={},
            postcode={}, 
            state={},
            country={}}

%% Abstract
\begin{abstract}
%% Text of abstract
Abstract text.
\end{abstract}


%%Research highlights
\begin{highlights}
\item Research highlight 1
\item Research highlight 2
\end{highlights}

%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

%% Use \section commands to start a section
\section{Introduction}
\label{sec1} %% Labels are used to cross-reference an item using \ref command.
The transportation sector represents the largest portion of greenhouse gas (GHG) emissions in the United States, which is approximately 28\% in 2022 (EPA). To reduce emission contributions by vehicles, electric vehicles (EVs) are in principle promising techniques that do not emit carbon dioxide ($CO_2$) emissions while traveling. Despite the advantages of the adoption of EVs such as lower mainenance cost (Blazquez and Moreno 2010), lower energy costs with high efficiency of electric drive(Singh et al. 2025), zero tailpipe emissions (Li et al. 2019), and lower air and noise pollution(Masri et al. 2024), the promotion of the dissemination of EVs to a large degree depends on the density of the EV charging network across urban areas and the resulting charging convenience (Shi et al. 2021). Sufficient EV charging infrastructures in an appropriate location attract potential/existing EV owners who can reduce range anxiety and are not far away from charging locations. Specifically, fast charging infrastructure can provide rapid charging options (e.g., 10 minutes to 1 hour to fully charge EVs) and can afford high charging demand. However, it requires substantial installation costs and may affect the reliability of the electricity supply. Recently, to cope with the increasing electricity demand through fast charging stations, alternative resources (e.g., solar energy) are chosen as primary electricity resources for EV charging, which connect with EVCSs. In this regard, it is important to develop the optimal planning of electric vehicle fast charging stations (EVFCS) with smart grids that can accommodate the growing charging demand while maintaining the reliability of the electricity supply. 

\vspace{0.5cm}

Current coverage of fast charging infrastructures has highly gaps at the state, county, and zip code level (Hanig et al. 2025, Roy and Law 2022). Most states and counties, except for California, Nevada, and Arizona, do not build out infrastructure, and these gaps are the same at the city level, resulting in the spatial inequities of EV charging access. In this respect, along with the increase in the importance of optimal planning of EVCS, researchers have focused on how to allocate the charging stations. Current studies have developed an optimal allocation planning of EVCS at station-level using a heuristic multi-objective approach. However, they tend to curtail the scope of the target area to the community (i.e., district) scale and utilize the graph data structure for the distribution network and road (traffic) network that only guarantees the accessibility of charging stations, where potential charging stations are forced close to those networks. In addition, existing approaches (e.g., heuristic multi-objective optimization approach) have still been challenged to converge to global-optimal solutions in the complex and stochastic environment with computational efficiency and scalability (Sharma and Kumar 2022).

\vspace{0.5cm}

This study aims to fill this knowledge gap by investigating a sequential decision-making-based learning approach for optimal allocation planning of EVFCS powered by smart grids using a multi-policy proximal policy optimization (PPO) deep reinforcement learning (DRL) approach. The PPO DRL approach, which is a class of policy gradient methods, can approximate the multiple optimizations from the stochastic unseen observation map by constraining the policy gradients. The DRL model investigates the optimal sites for the installation of EVFCS exploration in the veiled regions at a city scale and implements the geospatial analysis-based decision-making approach by optimizing the learning model stably and fast. This study is designed to learn the multidimensional complex urban environment that adapts agents to three different policies (e.g., environmental, economic, and urbanity). Specifically, this study proposes a two-stage investigation approach for allocating the charging stations in the city; 1) selecting the searching of start point to community in the city scale, and 2) finding the best place for the installation of EVFCS in the community scale, to make up for the weakness of RL, which is usually trained with small scale of image or grid state. This study also estimates the capacities of each potential charging station that can adjust the balance between environmental, economic, and urban factors. This approach allows for an in-depth understanding of how DRL-based geospatial analysis makes the optimal decision of the placement and size of EVFCS powered by solar photovoltaic (PV) plants. It also provides EVFCS allocation planning that can contribute to achieving the equitable accessibility of EVFCS for urban sustainability. 

\vspace{0.5cm}

\section{Research Background}

\subsection{Optimal EVCS allocation planning}
Optimal planning of EVCS is an investigation of optimal sites for the EVCS installation in a given region. The optimal sites are chosen by prioritizing among the candidate (i.e., alternative) sites. Specifically, extensive studies have expanded the location problem to the location-allocation (LA) problem to distribute charging stations in a given region with a satisfying objective. For example, (Liu et al. 2012) presented the optimal planning of EVCS connecting with distribution systems. They estimated charging types and sizing in pre-selected candidate sites, where the traffic network's layout requirements are satisfied, are close to load centers, and are convenient for accessibility. (Zeb et al. 2020) investigated optimal allocation sites by iteratively computing an objective function (minimization of installation cost and cost of power losses) while identifying the types of charger and EV penetration in the allocated sites. They started the optimization algorithm by randomly generating candidate sites in a huge search space. (Kazemtarghi et al. 2024) proposed a dynamic pricing strategy to maximize CSs' revenue. They simulated the distribution of selected CSs, where the sites are the highest selected by repeatedly charging EVs in fixed EV and CS locations. (Jordan et al. 2022) investigated a solution to the problem of locating a set of EVCSs in a city. They distributed charging stations in the point of interest (PoI), which are the attractive places for potential travelers. PoI is presented as the influence area is utilized to calculate the interference between new CS and existing CSs. Preplaced EVCSs from PoI analysis are updated within a given space, one sort of district in the city with estimating the size of EVCSs, where the utility of locating is maximized while minimizing the cost of its infrastructure. 

\vspace{0.5cm}

Allocating EVCS can contribute to alleviating the impact of EVCS on the reliability of the distribution network and maximizing of economic profit corresponding to the actual demand. However, existing studies have still challenged that the allocation of charging stations needs to be extended to examine coverage across the entire city. Their approach constrained the investigation areas at nodes with respect to the distribution network or transportation network. This method may not find potential optimal places, which are located in out of distribution or transportation networks while satisfying both demands (i.e., commercial area accessible with sub-road and powered by renewable resources). Also, they fixed the initial search points that may face a biased result and a less flexible model problem. In this respect, it is crucial to develop optimal allocation planning for EVFCS across the entire complex urban areas. 

\vspace{0.5cm}

\subsection{decision-making approach for EVCS planning}
Decision-making in computational science has been utilized to evaluate and develop the model that produces the optimal result from among the possible outcomes of various decisions. Specifically, DA is suitable for optimal planning of charging station placement that requires a framework to evaluate the problem from multiple perspectives (i.e., criteria, factors). The decision making can help to draw an adpative result efficiently and to improve it as more accurately. As one of the decision-making tools, a multi-criteria decision-making approach (MCDM) has been used to investigate the optimal sites for the installation of EVCS using geospatial analysis. MCDM refers to the support of choosing the best alternatives (e.g., cities) by considering multiple criteria. To prioritize among alternatives, each criterion is assigned a weight designed by pairwise Martics comparison. It is to compare between criteria to determine which one is the more important factor over another through fuzzy logic. This comparison is defined by an expert survey or literature reviews. Then, alternatives are ranked relying on the criteria weights using various MCDM approaches, such as the analytic hierarchy process (AHP) and technique for order preference by similarity to ideal solution (TOPSIS). The criteria weights for the best alternative can be utilized to create the suitability map on a geographic information system (GIS) that can be visually analyzed where determine the highest suitability for the installation of EVCSs. However, the MCDM approach has intrinsic limitations that the initial weights assigned by expert surveys can be subjective, leading to potential biases, criteria and alternatives are interdependent, and the method cannot estimate the sizing of potential charging stations.

To moderate these limitations, recent studies have used deep reinforcement learning (DRL) to investigate the optimal planning of EVCS considering multiple factors with estimating the size of charging stations. DRL-based decision-making approach sequentially explores all given areas while computing the relationships between multiple factors (i.e., criteria) that highly influence optimal planning by a deep neural network and identifies the optimal sites based on predefined criteria (i.e., reward function). For example, (Petratos et al. 2021) proposed the optimal placement planning of EVCS using deep reinforcement learning. They defined optimal sites as the charging demand is the highest area, which are computed by estimating EV charging demand through traffic data and POIs in a district scale. They compared the expected charging demand between the area selected by the model and a random area and showed the differences as a coefficient of determination ($R^2$) using the gradient boosting technique. (Ting et al. 2024) proposed a two-stage placement of mobile charging stations using multi-agent reinforcement learning. They represented the hexagonal grid map-based dynamic sitting of mobile charging stations in time steps as minimizing an inconvenience score (i.e., time cost) to handle the fluctuating charging demand with existing fixed charging stations using a heuristic algorithm. At each time step, the sum of convenience score with the demand of mobile and fixed CSs (i.e., utilization) for each mobile CSs is maximized using multi-agent deep reinforcement learning. (Huang and Zhou 2024) presented deep reinforcement learning-based placement of charging stations where the distance between potential charging stations and high traffic areas is minimized. Their model is supervised DRL by comparing the results with the normalized positions of historical datasets (e.g., high-traffic sites and existing charging stations). (Heo and Chang 2024) proposed the optimal planning of EVFCS using DRL and geospatial analysis. They spread out EVFCS across the entire city where five factors, such as the availability of EVFCS installation, balancing of charging demand and renewable resources' supply demand, and duplicated installation between potential charging stations and existing charging stations, are satisfied.  

\subsection{multipolicies optimization of EVFCS}

The single-objective optimization problem investigates the optimal planning of charging station infrastructure from an energy supply, demand response, or a charging demand response standpoint. Although a single-objective optimization appproach-based optimal planning of EVCS might benefit certain standpoint, it does not guarantee other important elements. In this regards, optimal placement of EVCS studies have investigated the optimal allocation planning of EVCSs using bi-level optimization model and multi-objective optimization model to consider EV user's preferences (behavior), supply demand response, and charging demand response. However, since the general multi-objective-based approaches only determine the CS placement in which all object functions are optimized, multi-objective optimization has been shifted to consider the weighting coefficients or trade-off of multiple restrictive and partially contradictory objectives. The reults of EVCS placement are varied by different constraints (e.g., demand, network, area scale, duplicate geometry to charging stations) and the weights of objectives (e.g., minimization of the number of charging stations for the cost, minimization of a balanced distance system between stations) (Yalcin et al. 2022). Specifically, recent studies applied a heuristic algorithm with multi-objective optimization, which is a problem-solving approach that aims to find good solutions, but does not guarantee an optimal solution, in the EVCS placement problem. Heuristic-based EVCS placement planning can present several solutions, representing the trade-off among the objective functions that lead to different placement plans. For example, (Zapotecas-Martinez et al. 2024) presented two evolutionary approaches for multi-objective optimization (e.g., multi-objective evolutionary algorithm based on decomposition (MOEA/D), and non-dominated sorting genetic algorithm \roman{2} (NSGA-\roman{2})) to three solutions from determining the optimal capacity and location of CSs, while computing the trade-off between the travel time and the number of stations. (Pourvaziri et al. 2024) presented a hybrid model for the EVCS location-capacity problem, combining artificial neural network (ANNs) and NSGA-\roman{2} multi-objective optimization approach. ANN is trained to estimate the waiting times of EVs in the charging network. The output is used to compute the Pareto optimal solutions of the trade-off between waiting time and cost in the multi-objective optimization. Specifically, they showed the trade-off results for 10 solutions that vary in cost, average waiting time, total number of charging stations, and total number of ports. (Panah et al. 2022) presented a multi-objective optimization approach to optimize profit, GHG emission, and voltage profile. They investigated the best solution among the ten candidate solutions using a MCDM approach, which computes the weight coefficients of three optimized factors for three CSs. Accordingly, the allocation of CSs can be varied depending on the different solutions (i.e., trade-off or weights between objectives), even if multi-objectives are converged to optimal points (i.e., minimizing/maximizing the objectives). 

\vspace{0.5cm}

Although the existing studies explained in the three sub-sections have gleaned meaningful results, sequential decision-making-based multi-objective optimal allocation planning of EVFCS across the urban areas remains understudied. Most related studies have focused on implementing either allocation planning, a sequential decision-making approach, or a multi-objective approach. It is clear that the EVFCS placement planning can be better when integrating those approaches into the planning model. This is because, in general, these approaches are mutually complementary to resolve the limitations of other approaches. For example, the limitations in node-based allocation planning, which are (1) the absence of analysis of other important factors and (2) limited coverage of searching areas, can be solved through sequential decision making with a geospatial analysis approach (Heo and Chang 2024). Moreover, multi-objective-based optimal planning struggled to handle trade-offs or conflicts between objectives, suggesting several solutions based on a heuristic approach or an MCDM approach. 

\vspace{5pt}

\section{Methodology - May 2}
This study aims to develop a sequential decision-making-based computational method using map-format datasets to integrate geospatial features into data-driven modeling of optimal allocation planning of EVFCS at an urban scale. The proposed optimal planning allocates charging stations across the entire city while satisfying multiple policies (e.g., environmental, economic, and urbanity factors). Specifically, we adopt a multi-agent deep reinforcement learning (DRL) with geospatial analysis that can independently optimize each objective (i.e., policy) by investigating complex and massive urban areas. Each policy suggests an optimal site for EVFCS while estimating its capacities where the criteria are all satisfied and is concatenated to the average of three results. This average can represent the optimal solutions (i.e., the highest average of three policies) and the sub-optimal solutions (i.e., satisfying two of three policies among three policies). Furthermore, this study suggests a two-phase of site selection approach that allows agents to investigate the entire city equally; (1) selecting a community area (77) in the city by the kernel density-based probability, (2) randomly selecting the starting point within the community area. 

\vspace{5pt}

Fig. xx illustrates the overall scheme of optimal allocation of public EVFCS using the multi-agent DRL algorithms over three parts; (1) data collection and generation, (2) DRL allgorithm, and (3) result analysis and visualization. In the data collection and generation step, eight dataests are collected on Geographic information System (GIS): (1)... The spatial dataset 

\subsection{Multi-agent Deep Reinforcement Learning}

\subsubsection{Traditional Reinforcement learning}
Reinforcement learning (RL) is a sequential decision-making mechanism that an agent learns to make proper decisions in response to the environment by trial and error. Under the Markov decision process, the Agent takes action ($a_t$) for a state ($s_t$) over a series of discrete time steps ($t$) $\mathrm{P}=\mathbb{P}[S_{t+1}=s'|S_t=s, A_t=a]$. The agent receives the feedback (i.e., reward ($r_t$)) for its actions, which can be positive if the action is proper. The agent builds a set of rules, named a Policy ($\pi_t$) $\pi(a|s) = \mathbb{P}[A_t=a|S_t=s]$, that computes the probability of actions over states. The agent's goal is to maximize cumulative rewards (i.e., total discounted rewards) $G_t=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$, balancing between known action-state pairs for high immediate rewards (i.e., exploitation) and new state-action pairs for better future rewards (i.e., exploration) using discount factor gamma $\gamma$. Specifically, to compute the expected cumulative discounted future reward of the state ($s_t$) according to a stochastic policy $\pi$, the state-value function ($v_\pi(s)$) for policy $\pi$ is defined formally by v_\pi(s) \doteq \mathbb{E}_\pi[G_t|S_t=s] = \mathbb{E}_\pi\left[ \sum_{k=0}^{\infty } \gamma^kR_{t+k+1} | S_t=s \right], for all $s \in S$. Similarly, the action-value function ($q_\pi$) for the policy is defined by q_\pi(s,a) \doteq \mathbb{E}_\pi[G_t|S_t=s, A_t=a] = \mathbb{E}_\pi\left[ \sum_{k=0}^{\infty } \gamma^kR_{t+k+1} | S_t=s, A_t=a \right]. The reinforcement learning objective, which is to achieve the highest average return for each state, can be computed by parameterizing the optimal state-value function ($v_*(s)$) for optimal policies ($q_*(s,a)$), and it can combine two functions into the Bellman optimality equation $v_*(s) = \underset{a}{\mathrm{max}}\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]$ = q_*(s,a).

\subsubsection{Environment}
Reinforcement learning has the four essential components: agent, action, observation(state), and reward. 
The environment provides information that helps the agent determine which actions to take. Over learning, the agent learns optimal decision making from rewards within the environment that satisfy a specified goal. The goal of this research is to investigate the optimal sites for EVFCS at an urban scale, and so the environment in this study represents urban areas (e.g., Chicago) as a large grid format, including various types of geospatial data, which are categorized into three criteria: environmental, economic, and urbanity. Based on the literature review, this study constructs xx layers of the environment matrix utilizing xx geospatial data: tree canopy,  traffic density, road network, distribution network, land use, and potential electricity output. 

First, for the environmental factor, this study considers the deterioration of the green and the expected $CO_2$ reduction.  

\subsubsection{Agent and action}
This study uses a multi-agent deep reinforcement learning algorithm in which each agent investigates optimal sites of EVFCS according to different criteria (i.e., environmental, economic, and urbanity). All three agents take the same types of three continuous actions on rows, columns, and capacities of CSs. Policies trained from DRL output three actions, as normalized to the range of -1 to 1 for stable learning. Then, normalized actions are inversely transformed to -50 to +50 for row and column actions, which moves to the next state toward the optimal site. 

Furthermore, the capacities of charging stations are defined by considering the balancing with supply and charging demand, the effectiveness of reducing $CO_2$ emissions, and the cost function. The range of capacities is 2 to 12, which is the same as 6 MW/day to 72 MW/day by $6,000 * (\mathrm{action[2] * (\mathrm{max \: capacity} - \mathrm{min \: capacity})/2 + (\mathrm{max \: capacity} + \mathrm{min \: capacity})/2)}$. All stalls are assumed to be V2 Tesla Superchargers, which can deliver energy to a connected EV at a rate of 250 kW per hour (i.e., 1 stall = 250 kWh = 6 MW/day, 12 stalls = 72 MW/day). 

\subsubsection{Observation and state}
Traditional RL assumes that the agent has complete access to all information from the environment at every location in time as fully observable environment. Although a fully observable environment can learn the optimal decision-making through tracking of all relevant information, it does not work for complex and large scale of environment and may fail to generalize to unseen data with a risk of overfitting. To explore large and complex of entire urban areas in this study, this study utilizes a partially observable environment method, which can access to partial information about limited environment at specific location in time. This study defines the observation by extracting the environment map that provides important geospatial information, enabling agents to develop optimal strategies to achieve objectives at a specific point in time. The observation is extracted into a 100 x 100 grids (e.g., 1 grid is equal to 100m) in row-column coordinates and agents (i.e., potential CS) are located in the centroid of the observation (50, 50) coordinate. 

Unlike the previous study (Heo and Chang 2024) that directly uses observation to state, this study shrinks the observation into the state, which includes ten information from it, due to a high-dimensional environment map, which overlays eight geospatial layers, it may lead to arise poor performance of learning model with increasing computational load. The state consists of [rows, columns, community boundary, landuse, \alpha, the minimum distance to the power grid, the minimum distance to the roads, average of the vegetation canopy in the observation, VMT, potential electricity, the average distance to the existing potential stations within the observation].

\subsubsection{Reward function}
The reward function is a critical component that significantly influences the performance and learning efficiency of RL agents to achieve goals. Specifically, this study utilizes MARL in that each agent is an independent entity. Each agent sets up a different reward function applying both the MCDM category approach and the multi-objective optimization approach in building reward functions of three agents, as shown in Table xx. The policies from the agents are categorized into three main criteria (e.g., environment, economy, urbanity) with xx sub-criteria. Each agent investigates an optimization point in each main criterion using DRL, such as maximizing the profit benefit in the economic policy.

\begin{tabular}{>{\raggedright\arraybackslash}p{4cm} >{\raggedright\arraybackslash}p{5.5cm} >{\raggedright\arraybackslash}p{5.5cm}}
\toprule
\textbf{Main rewards / Sub rewards} & \textbf{Definition} & \textbf{Reward function} \\
\midrule

\textbf{Environment Rewards} & 
$R_1 = \dfrac{r_{avm} + r_{viss} + r_{eser}}{3}$ &
\[
R_1 = 
\begin{cases}
+10, & \text{if } R_1 \geq 0.6 \\
-1, & \text{if } R_1 < 0.6
\end{cases}
\] \\
\quad $r_{avm}$ & & \\
\quad $r_{viss}$ & & \\
\quad $r_{eser}$ & & \\

\midrule

\textbf{Economic Rewards} & 
$R_2 = (P_G \times 12) - F_z$ &
\[
R_2 = 
\begin{cases}
+10, & \text{if } R_2 \geq 0 \\
-1, & \text{if } R_2 < 0
\end{cases}
\] \\
\quad $P_G$ & & \\
\quad $F_z$ & & \\

\midrule

\textbf{Urbanity Rewards} & 
$R_3 = \dfrac{r_{drn} + r_{dg} + r_{lu} + r_{sc}}{4}$ &
\[
R_3 = 
\begin{cases}
+10, & \text{if } R_3 = 1 \\
-1, & \text{if } R_3 < 1
\end{cases}
\] \\
\quad $r_{drn}$ & & \\
\quad $r_{dg}$ & & \\
\quad $r_{lu}$ & & \\
\quad $r_{sc}$ & & \\

\bottomrule
\end{tabular}


In the environment policy, $R_1$ evaluates the point of minimization of vegetation deterioration when a charging station is installed and the point of maximization of reducing $CO_2$ emissions by travel and energy resources corresponding to charging demand. Specifically, there are two sub-criteria for computing vegetation deterioration: 1) the average of the vegetation ratio in the observation map, and 2) the degree of vegetation deterioration at the position of the agent. For example, when the agent is located where the average of the vegetation ratio ($avm$) is 0.40 and the ratio of vegetation canopy ($viss$) is 0.50, the degree of vegetation deterioration $r_{avm}$ is 1.28, $e^{-avm}$ + $e^{-viss}$. Furthermore, the total reduction of $CO_2$ emission ($r_{TER}$) by EVs can be calculated by subtracting the reduction of $CO_2$ emissions ($r_{apr}$) by EVs while driving $\mathrm{raw\:VMT} * \frac{23.7 (\mathrm{lbs.CO_2e/1gal})}{21.79 (\mathrm{mi/1gal}))}$ from the electricity sources emissions with replaced alternative resources in total electricity generation ($r_{eser}$) $(1-\alpha)*\mathrm{raw\:VMT} * 0.72576 / 4.56$. This study assumed that EVs are Tesla Model 3 (electricity efficiency of $4.56 \: \mathrm{mi/kWh}$ and average $CO_2$ emissions from generated electricity in Illinois $0.72576 \: \mathrm{lbs. \: CO_2e/kWh}$. The total reduction of $CO_2$ emissions ($r_{TER}$) is rescaled to match with sub-criteria of the vegetation deterioration $r_{avm}$, $1-e^{-r_{TER}}$. The total reward of the environment policy is averaged to two sub-criteria of the degree of vegetation deterioration and the total $CO_2$ reduction by EVs. The agent receives $R_1$ of 1 for the average of two sub-criteria is more than 0.6. 

In the economic policy, $R_2$ explores the payback period by maximizing the profit of the EV charging network by balancing the capacities of charging stations and charging demand. Specifically, payback can be calculated by extracting the profitability of the charging station ($P\_G$) with an expected payback period (12 years) from the cost function, which sums all the input costs of the charging facility ($F\_z$). The yearly profitability of the charging station ($P\_G$) is only obtained from the charging $\alpha*(0.28-0.06) + (1-\alpha)*(0.28-0.0405)) * VMT * 365$, which sums the margin of the average sales of Tesla Supercharger (0.28 \$ kWh) with charging price of alternative resource (0.06\$/kWh) and commercial electricity (0.0405\$/kWh in Chicago) provided by the grid, $\mathrm{the \: number \: of \: stalls} * \$20,600 * 0.2 + 800$. Also, the shared cost of a charging station $F$ sums the installation cost of fast stalls $C$ (average \$20,600 per stall (150kW)) with incentive (i.e., up to 80\%), and the maintenance and management cost coefficient $\rho$ (\$800/charger/year), multiplying with the number of stalls ($z$), $F = z * (20,600*0.2 + 800)$. $R_2$ receives a + 10 reward if the profitability exceeds the cost within the payback period.

In the urbanity policy, $R_3$ evaluates the overall usability of EVFCSs in the target area. The $R_3$ receives positive rewards (+10) when four sub-criteria are satisfied: (1) availability of construction of the EVFCS ($r\_drn$), (2) accessibility to the EVFCS ($r\_dg$), (3) accessibility to the distribution network or solar plants installed in the rooftop of buildings ($r\_lu$), and (4) affordability of charging demand to the sizing of charging stations ($r\_sc$). For instance, $r\_drn$ receives a penalty of 0 when the agent is located in an unavailable position for the installation of a charging station, such as residential, institutional, prohibited, and miscellaneous areas. $r\_dg$ involves the proximity of roads to the agent by calculating the Euclidean distance between them. If the distance is within a 5 km radius of the agent's position, $r\_dg$ receives a compensation of +1. $r\_lu$ explores the proximity of the distribution network or solar power plants to the agent through the Euclidean distance method. For example, $r\_lu$ receives +1 reward for sufficiently affordable the charging demand (i.e., $\alpha = 1$), +0.5 for partially affordable the charging demand (i.e., $0<\alpha<1$), and 0 for non-installation of solar power plants at near (i.e., $\alpha=0$). $r\_sc$ evaluates whether the predicted sizing of the agent taking the action can afford the charging demand (i.e., VMT). $r\_sc$ receives a positive reward (+1) if the predicted capacity of EVFCS is more than the charging demand ($\mathrm{VMT}/4.56$ MW). 

\subsubsection{Policy Gradient Method}
The policy gradient method aims to model and optimize the policy directly to achieve RL's main goal, which is to explore an optimal policy for the agent to obtain optimal rewards. The optimal policy can be approximated by parameterizing the policy parameter $\theta$, $\pi_\theta(a|s)$ based on the gradient of the scalar performance measure $\nabla _{\theta}J(\theta)= \sum_{s}^{}\nabla_\theta\log\pi_\theta(a|s)R(\tau)$, Equation xx, that denotes cumulative reward $R(\tau)$ in the direction of the steepest increase of the probability of selecting action at state $s$ $\sum_{s}^{}\nabla_\theta\log\pi_\theta(a|s)$. For example, if the return is high, the network may push up the probabilities of the ($s_t$,$a_t$) pairs. However, this method has challenges that the variance is high, since the updates are highly dependent on the trajectories without a true estimation of the return, which can lead to different returns in the same state across episodes. 


The actor-critic methods learn approximations to both policy and value function that can predict the policy by only considering the input state: (i) 'actor' refers to the learned policy to update the policy parameter $\pi_\theta(a|s)$, in the direction suggested by the critic, and (ii) 'critic' refers to the learned value function to update the value function shared with the actor network in this study. Advantage Actor Critic (A2C) algorithm updates the parameter by comparing the estimated expected future reward for an action in a state ($Q(s,a)$) with the average value of the same state ($V(s)$,) called advantages $A(s,a)=Q(s,a) - V(s)=r+\gamma V(s')-V(s)$. For example, if the advantage value is higher than 0, the gradient will be moved in the positive direction. The advantages $(A_t)$ can mitigates the variance of the policy gradient by controlling updates within the average value of the state. For continuous actions, the action can be taken by a normal Gaussian distribution after predicting the policy in the parameterization $\pi_\theta(a|s)\doteq \frac{1}{\sigma(s,\theta)\sqrt{2\pi}}\mathrm{exp}(-\frac{(a-\mu_\theta(s))^2}{2\sigma_\theta(s)^2})$. It can allow the policy to take continuous actions under the probability density function for the exploration-exploitation trade-off. For example, we can take three actions under the Gaussian distribution (\mu,\sigma), \mu denotes the range of -1 to +1 in the hyperbolic tangent activation function in the last layer of the actor network, and \sigma denotes the fixed value of 0.5. 

The main challenge of the A2C algorithm is that A2C aggressively updates the policy after every step, without any threshold for the model parameter. It suffers convergence problem, causing vanishing or exploding gradients with poor sample efficiency. To handle this challenge in the policy gradient problem, the proximal policy optimization (PPO) approach utilizes a clipped surrogate objective, drawing from the trust region policy optimization approach. The clipped surrogate objective maximizes an expected discounted reward on a conservative policy iteration, $a$, using KL-divergence, which calculates the probability ratio $r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\mathrm{old}(a_t|s_t)}}$ between the old policy and the current policy. For example, if the probability ratio is more than 1, the action in the state is closer to the current policy than the old policy. Also, it constrains unstable policy updates through a clipping mechanism that takes the minimum of the clipped ($\mathrm{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A_t}$) and unclipped objective ($r_t(\theta)\hat{A_t}$. Finally, the surrogate objective can prevent a penalty for having large updates. 

\subsubsection{System Model}
This study considers the optimal allocation planning of EVFCSs over episodes that represent the possible areas in which three agents receive positive rewards. Each agent has an independent policy (i.e., neural network) and a placeholder, which stores important information, such as the records of action and reward. Unlike a typical reinforcement learning environment that resets the environment at the beginning of each episode, the global environment in this study does not reset the environment but updates the information, such as the VMT, potential electricity, CS's position, and vegetation canopy, after all agents in each episode. At each episode $e$, we select the community area (i.e., 77 in Chicago) to prevent unequally allocating the EVFCSs at a large-scale environment. Especially, in the beginning of the learning, the selection is ruled by the equal distribution probability (i.e., 1/77) until collecting the proper sampling of potential charging stations (i.e., 30), then the selection method is shifted to the Kernel density-based distribution probability, as shown in the Equation. The Kernel density computes how many potential charging stations are installed in the surrounding centroid of communities that the radius of Kernel varies depending on the area of the communities. Then, the Equation xx. transform the Kernel density into the probability format for all the communities. 

The agent randomly selects the starting point within the selected community, as shown in Figure xx. At each time step $t$, the agent determines the positions and the capacities of EVFCSs. After the three agents finished exploring the optimal site, this study can determine which combinations of the results of the three agents are the optimal solution, the sub-optimal solution, or the nonsolutions. For example, this study refers: (1) an optimal solution if all agents receive a positive reward, (2) a sub-optimal solution if two of three agents receive a positive reward, and (3) nonsolution if one of three or none agents receive a positive reward. Then, the environment is updated, only for agents that achieve either an optimal or sub-optimal solution, as illustrated in the Figure xx. 

\section{Results - By May 11}
Figures

\section{Discussions - May 15}
Importance of normalization
tangent 

Not learning - stability issues - rewards revisions - still not learning, but tendancy exists, and still dramatic results - back to records - rewards' issues - can be resolved by hyperparameter tuning 


%% Use \subsection commands to start a subsection.
\subsection{Example Subsection}
\label{subsec1}

Subsection text.

%% Use \subsubsection, \paragraph, \subparagraph commands to 
%% start 3rd, 4th and 5th level sections.
%% Refer following link for more details.
%% https://en.wikibooks.org/wiki/LaTeX/Document_Structure#Sectioning_commands

\subsubsection{Mathematics}
%% Inline mathematics is tagged between $ symbols.
This is an example for the symbol $\alpha$ tagged as inline mathematics.

%% Displayed equations can be tagged using various environments. 
%% Single line equations can be tagged using the equation environment.
\begin{equation}
f(x) = (x+a)(x+b)
\end{equation}

%% Unnumbered equations are tagged using starred versions of the environment.
%% amsmath package needs to be loaded for the starred version of equation environment.
\begin{equation*}
f(x) = (x+a)(x+b)
\end{equation*}

%% align or eqnarray environments can be used for multi line equations.
%% & is used to mark alignment points in equations.
%% \\ is used to end a row in a multiline equation.
\begin{align}
 f(x) &= (x+a)(x+b) \\
      &= x^2 + (a+b)x + ab
\end{align}

\begin{eqnarray}
 f(x) &=& (x+a)(x+b) \nonumber\\ %% If equation numbering is not needed for a row use \nonumber.
      &=& x^2 + (a+b)x + ab
\end{eqnarray}

%% Unnumbered versions of align and eqnarray
\begin{align*}
 f(x) &= (x+a)(x+b) \\
      &= x^2 + (a+b)x + ab
\end{align*}

\begin{eqnarray*}
 f(x)&=& (x+a)(x+b) \\
     &=& x^2 + (a+b)x + ab
\end{eqnarray*}

%% Refer following link for more details.
%% https://en.wikibooks.org/wiki/LaTeX/Mathematics
%% https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics

%% Use a table environment to create tables.
%% Refer following link for more details.
%% https://en.wikibooks.org/wiki/LaTeX/Tables
\begin{table}[t]%% placement specifier
%% Use tabular environment to tag the tabular data.
%% https://en.wikibooks.org/wiki/LaTeX/Tables#The_tabular_environment
\centering%% For centre alignment of tabular.
\begin{tabular}{l c r}%% Table column specifiers
%% Tabular cells are separated by &
  1 & 2 & 3 \\ %% A tabular row ends with \\
  4 & 5 & 6 \\
  7 & 8 & 9 \\
\end{tabular}
%% Use \caption command for table caption and label.
\caption{Table Caption}\label{fig1}
\end{table}


%% Use figure environment to create figures
%% Refer following link for more details.
%% https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions
\begin{figure}[t]%% placement specifier
%% Use \includegraphics command to insert graphic files. Place graphics files in 
%% working directory.
\centering%% For centre alignment of image.
\includegraphics{example-image-a}
%% Use \caption command for figure caption and label.
\caption{Figure Caption}\label{fig1}
%% https://en.wikibooks.org/wiki/LaTeX/Importing_Graphics#Importing_external_graphics
\end{figure}


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix
\section{Example Appendix Section}
\label{app1}

Appendix text.

%% For citations use: 
%%       \citet{<label>} ==> Lamport [21]
%%       \citep{<label>} ==> [21]
%%
Example citation, See \citet{lamport94}.

%% If you have bib database file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-num-names} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

\begin{thebibliography}{00}

%% For authoryear reference style
%% \bibitem[Author(year)]{label}
%% Text of bibliographic item

\bibitem[Lamport(1994)]{lamport94}
  Leslie Lamport,
  \textit{\LaTeX: a document preparation system},
  Addison Wesley, Massachusetts,
  2nd edition,
  1994.
\bibitem[PlugInAmerica(2023)]{PlugInAmerica2023}
    Plug in America,
    \textit{\LaTeX: 2023 EV Driver Survey: A Strong Year for EVs, But Charging Reliability Needs Improvement},
    Plug In America,
    http://pluginamerica.org/wpcontent/uploads/2023/05/2023-EV-Survey-Final.pdf,
    2023.
\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.
